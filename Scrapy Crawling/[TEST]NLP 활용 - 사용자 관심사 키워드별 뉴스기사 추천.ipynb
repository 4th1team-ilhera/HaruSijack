{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [NLP 활용] 언론사별 상위 10개 뉴스기사 요약본 안내"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "언론사별 뉴스 크롤링: 100%|██████████| 81/81 [01:46<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘의 주요 키워드:\n",
      "사망: 21\n",
      "중대장: 16\n",
      "입건: 16\n",
      "조사: 14\n",
      "정부: 14\n",
      "사건: 14\n",
      "훈련병: 13\n",
      "경찰: 12\n",
      "\n",
      "추천 뉴스:\n",
      "1. “학교 X 같은 사람”…대학축제 무대서 욕한 비비 사과, 무슨일이 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/009/0005316389?ntype=RANKING\n",
      "2. “브레이크 고장났다”…경주서 페라리 몰다 앞차와 추돌한 日괴짜부호 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/009/0005316276?ntype=RANKING\n",
      "3. “우리 아빠가 윤석열 나쁜 사람이래요”…조국이 전한 강원도 민심? (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/009/0005316302?ntype=RANKING\n",
      "4. “몰래 낳았는데 울면 들킬까봐”...신생아 밟아 죽인 비정한 20대 미혼모 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/009/0005316372?ntype=RANKING\n",
      "5. “처벌 안받아도 평생 반성”...임창정, 주가조작 연루 무혐의 심경 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/009/0005316338?ntype=RANKING\n",
      "6. 비트코인 박스권 탈출, 마지막 고비?… 이번주 美 FOMC·5월 CPI 발표 [DD주간브리핑] (NEGATIVE)\n",
      "   Link: https://n.news.naver.com/article/138/0002175012?ntype=RANKING\n",
      "7. [DD's톡] 연일 지붕 뚫고 하이킥…삼양식품·카페24 질주 이유 들여다보니 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/138/0002174982?ntype=RANKING\n",
      "8. 디도스 때문에 못 살겠네… T1의 간절한 SOS [e모션] (NEGATIVE)\n",
      "   Link: https://n.news.naver.com/article/138/0002175043?ntype=RANKING\n",
      "9. K-배터리 3사, 非중국 배터리 시장 합산 점유율 46.7%…전년비 1.6%p↓ [소부장박대리] (NEGATIVE)\n",
      "   Link: https://n.news.naver.com/article/138/0002175006?ntype=RANKING\n",
      "10. [인터뷰] “매크로 차단, 티케팅 산업만의 문제 아냐…안정성·비용절감 위한 선택” (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/138/0002174981?ntype=RANKING\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Server에 부하를 주지 않기 위한 Crawling 속도 제한용 Library Package\n",
    "import time\n",
    "import random\n",
    "# Crawling 진행 상황을 체크하기 위한 Module\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from collections import Counter\n",
    "\n",
    "# 뉴스 Crawling\n",
    "def get_news_links_by_press (url) :\n",
    "  headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "  response = requests.get(url, headers = headers)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  \n",
    "  press_data = {}\n",
    "  press_sections = soup.select('.rankingnews_box')\n",
    "  \n",
    "  for press_section in tqdm(press_sections, desc = \"언론사별 뉴스 크롤링\") :\n",
    "    press_name = press_section.select_one('.rankingnews_name').get_text(strip = True)\n",
    "    news_links = set()  # 중복 제거를 위한 set 사용\n",
    "    for item in press_section.select('li a') :\n",
    "      title = item.get_text(strip = True)\n",
    "      link = item['href']\n",
    "      if title and link and \"동영상\" not in title :  # 타이틀이 존재하고 \"동영상\"이 포함되지 않은 경우에만 추가\n",
    "        news_links.add((title, link))\n",
    "    press_data[press_name] = list(news_links)[:5]  # 다시 list로 변환 후 상위 5개만 저장\n",
    "    \n",
    "    # 각 언론사별 뉴스 Crawling 후 대기 시간 추가\n",
    "    time.sleep(random.uniform(0.5, 2.0))\n",
    "  \n",
    "  return press_data\n",
    "\n",
    "# Data 전처리\n",
    "def preprocess_text (text) :\n",
    "  okt = Okt()\n",
    "  # tokens = okt.morphs(text, stem = True)\n",
    "  tokens = okt.nouns(text)  # 명사만 추출\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# 감성 분석 모델과 토크나이저를 미리 다운로드\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 감성 분석 파이프라인 초기화\n",
    "classifier = pipeline('sentiment-analysis', model = model, tokenizer = tokenizer)\n",
    "\n",
    "# 감성 분석 함수\n",
    "def sentiment_analysis (text) :\n",
    "  return classifier(text)\n",
    "\n",
    "# Topic 모델링\n",
    "def topic_modeling(docs, num_topics = 5) :\n",
    "  vectorizer = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')\n",
    "  dtm = vectorizer.fit_transform(docs)\n",
    "  lda = LatentDirichletAllocation(n_components = num_topics, random_state = 0)\n",
    "  lda.fit(dtm)\n",
    "  return lda, vectorizer\n",
    "\n",
    "# 주요 Keyword 추출\n",
    "# def extract_keywords (docs, num_keywords = 10) :\n",
    "#   words = ' '.join(docs).split()\n",
    "#   counter = Counter(words)\n",
    "#   return counter.most_common(num_keywords)\n",
    "def extract_keywords(docs, num_keywords = 10) :\n",
    "  words = ' '.join(docs).split()\n",
    "  counter = Counter(words)\n",
    "  # 불용어 제거\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  keywords = [(word, freq) for word, freq in counter.most_common(num_keywords) if word not in stop_words and len(word) > 1]\n",
    "  return keywords\n",
    "\n",
    "# 메인 함수\n",
    "def main () :\n",
    "  base_url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "  press_news_data = get_news_links_by_press(base_url)\n",
    "  \n",
    "  # 뉴스 DataFrame 생성\n",
    "  news_list = []\n",
    "  for press_name, news_data in press_news_data.items() :\n",
    "    for title, link in news_data :\n",
    "      news_list.append([press_name, title, link])\n",
    "  df = pd.DataFrame(news_list, columns=['Press', 'Title', 'Link'])\n",
    "  \n",
    "  # Data 전처리\n",
    "  df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
    "  \n",
    "  # 감성 분석\n",
    "  df['Sentiment'] = df['Title'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "  \n",
    "  # Topic 모델링\n",
    "  lda, vectorizer = topic_modeling(df['Processed_Title'].tolist())\n",
    "  topics = lda.components_\n",
    "  feature_names = vectorizer.get_feature_names_out()\n",
    "  topic_keywords = []\n",
    "  for topic_weights in topics :\n",
    "    top_keywords = [feature_names[i] for i in topic_weights.argsort()[:-11:-1]]\n",
    "    topic_keywords.append(top_keywords)\n",
    "  \n",
    "  # 주요 Keyword 추출\n",
    "  keywords = extract_keywords(df['Processed_Title'].tolist())\n",
    "  \n",
    "  # 결과 출력\n",
    "  print(\"오늘의 주요 키워드:\")\n",
    "  for keyword, freq in keywords :\n",
    "    print(f\"{keyword}: {freq}\")\n",
    "  \n",
    "  print(\"\\n추천 뉴스:\")\n",
    "  for idx, row in df.head(10).iterrows() :  # 상위 10개의 뉴스만 출력\n",
    "    print(f\"{idx + 1}. {row['Title']} ({row['Sentiment']})\")\n",
    "    print(f\"   Link: {row['Link']}\")\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
