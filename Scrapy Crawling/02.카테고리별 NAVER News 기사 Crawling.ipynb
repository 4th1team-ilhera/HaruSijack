{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling NAVER News Articles by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library Package for Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm  # tqdm Moduel은 진행 상황을 체크하기 위한 Moudel임.\n",
    "\n",
    "# Server에 부하를 주지 않기 위한 Crawling 속도 제한용 Library Package\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한 Category와 Page에서 뉴스 기사 Link 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC로 NAVER News를 들어가면 기사들이 정치, 경제, 사회, 생활/문화, IT/과학, 세계 등의 6가지 Category로 분류되어 있는 것을 확인할 수 있습니다. <br><br>\n",
    "정치 Category의 Link는\n",
    "  <a href = \"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100\">\n",
    "    <span style = \"color : #FFBE98\">\"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100\"\n",
    "  </a>\n",
    "인 것을 확인할 수 있는데 <br><br>\n",
    "Link 뒤쪽의 sid1변수에 할당된\n",
    "  <span style = \"color : #FFBE98\">**변수 100이 정치 Category**의 번호입니다.</span> <br><br>\n",
    "이와 마찬가지 방법으로 확인해 보면\n",
    "  <span style = \"color : #FFBE98\">**경제는 101, 사회는 102, 생활/문화는 103, 세계는 104, IT/과학은 105로 분류**</span>\n",
    "되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ex_tag (sid, page) :\n",
    "#   ### 뉴스 분야(sid)와 페이지(page)를 입력하면 그에 대한 Link들을 List로 추출하는 함수 ###\n",
    "  \n",
    "#   ## 1.\n",
    "#   url = f\"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={sid}\"\\\n",
    "#   \"#&date=%2000:00:00&page={page}\"\n",
    "#   html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "#   \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "#   \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "\n",
    "#   soup = BeautifulSoup(html.text, \"lxml\")\n",
    "#   a_tag = soup.find_all(\"a\")\n",
    "  \n",
    "#   ## 2.\n",
    "#   tag_lst = []\n",
    "#   for a in a_tag :\n",
    "#     if \"href\" in a.attrs:  # href가 있는것만 고르는 것\n",
    "#       if (f\"sid={sid}\" in a[\"href\"]) and (\"article\" in a[\"href\"]) :\n",
    "#         tag_lst.append(a[\"href\"])\n",
    "\n",
    "#   return tag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_tag (sid) :\n",
    "  ### 뉴스 분야(sid)를 입력하면 그에 대한 Link들을 List로 추출하는 함수 ###\n",
    "  \n",
    "  ## 1.\n",
    "  url = f\"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1={sid}\"\n",
    "  html = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n",
    "  \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "  \"Chrome/110.0.0.0 Safari/537.36\"})\n",
    "\n",
    "  soup = BeautifulSoup(html.text, \"lxml\")\n",
    "  a_tag = soup.find_all(\"a\")\n",
    "  \n",
    "  ## 2.\n",
    "  tag_lst = []\n",
    "  for a in a_tag :\n",
    "    if \"href\" in a.attrs:  # href가 있는것만 고르는 것\n",
    "      if (f\"sid={sid}\" in a[\"href\"]) and (\"article\" in a[\"href\"]) :\n",
    "        tag_lst.append(a[\"href\"])\n",
    "\n",
    "  return tag_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위의 함수는 뉴스 분야(sid)와 페이지(page)를 입력하면 그에 대한 링크들을 리스트로 추출하는 함수입니다.\n",
    "\n",
    "1. f-String을 사용하여 함수의 Parameter인 sid와 page를 할당받을 수 있는 url을 Variable(변수)에 저장합니다. <br><br>\n",
    "2. 이후, requests와 BeautifulSoup를 사용하여 해당 url의 html을 Parsing 합니다. <br><br>\n",
    "3. 그리고, find_all 함수를 사용하여 Parsing 된 html에서 HyperLink를 정의하는 a Tag를 모두 가져옵니다. <br><br>\n",
    "4. 가져온 a Tag들에 HyperLink가 걸려있지 않거나 뉴스 기사가 아닌 Link들이 섞여있을 수 있습니다. <br><br>\n",
    "> 따라서, if문으로 a Tag의 속성에 \"href\"가 속해있고, HyperLink 주소에 sid Number와 \"article\"이 포함되어 있는 Link만을 수집하여 tag_lst에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex_01)\n",
    "- <span style = \"color : #92A8D1\">**sid 넘버가 100인 url**에서 수집한 신문 기사의 Link 추출하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_1st = ex_tag(100)\n",
    "\n",
    "tag_1st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위의 방법은 현재 사용이 불가능하다.\n",
    "💡 NAVER News Link가 변동됨에 따라 다른 방법을 사용해야 한다.\n",
    "\n",
    "- 그러면 \"https://news.naver.com/section/{카테고리 분류 코드}\" 이 페이지 내에 있는 각각의 url을 수집한 다음에 기사 본문 등을 수집하고 <br><br>\n",
    "- \"기사 더보기\" 등의 버튼을 통해 추가적으로 기사를 수집하는 작업이 필요할 것 같은데, <br><br>\n",
    "  - <span style = \"color : #FFBE98\">이를 위해서는 \"**selenium**\"을 사용하는 것이 좋을 것 같습니다.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
