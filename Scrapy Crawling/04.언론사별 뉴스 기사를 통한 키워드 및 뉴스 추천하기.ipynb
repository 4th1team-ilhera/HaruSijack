{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Projectr : 하루시작 지하철 혼잡도 분석\n",
    "### Description : \n",
    "- <a><span style = \"color : #FFBE98\">**오늘의 주요 키워드 및 뉴스 추천하기**</a>\n",
    "    \n",
    "### Author : Zen Den\n",
    "### Date : 2024. 06. 11. (Tue) ~\n",
    "### Detail : \n",
    "### Update: \n",
    "- 2024.06.11. (Tue) K.Zen : \n",
    "  <a href = \"https://news.naver.com/main/ranking/popularDay.naver\">\n",
    "    <span style = \"color : #F7CAC9\">**NAVER 랭킹뉴스**</a>\n",
    "에서 언론사별 5개씩 가져오기\n",
    "  <span style = \"color : #FFBE98\">**(언론사명, Title, Link)**</span>\n",
    "  <br><br>\n",
    "- 2024.06.12. (Wed) K.Zen : <br><br>\n",
    "  - Selenium Library를 이용한 크롤링 과정 중 ScreenShot 촬영을 백그라운드로 실행하기 <br>\n",
    "  [참고자료]\n",
    "    <a href = \"https://co-de.tistory.com/21\">\n",
    "      <span style = \"color : #F7CAC9\">**[selenium]** 손쉽게 브라우저 자동 캡쳐 기능 만들기</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analyse News Articles by media outlet to recommend <br> the top keywords and news of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # 인터넷에서 Data를 가져오기 위한 Library\n",
    "from bs4 import BeautifulSoup # 웹 페이지 내용을 분석하기 위한 Library\n",
    "import time # 대기 시간을 추가하기 위한 Library\n",
    "import random # Random한 대기 시간을 만들기 위한 Library\n",
    "from tqdm import tqdm # Crawling 진행 상황을 체크하기 위한 Module\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAVER News - 언론사별 랭킹뉴스 Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언론사, Title, Link 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 Crawling\n",
    "def get_news_links_by_press (url) :\n",
    "  headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "  response = requests.get(url, headers = headers)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  \n",
    "  press_data = {}\n",
    "  press_sections = soup.select('.rankingnews_box')\n",
    "  \n",
    "  for press_section in tqdm(press_sections, desc = \"언론사별 뉴스 Crawling\") :\n",
    "    press_name = press_section.select_one('.rankingnews_name').get_text(strip = True)\n",
    "    news_links = set()  # 중복 제거를 위한 set 사용\n",
    "    for item in press_section.select('li a') :\n",
    "      title = item.get_text(strip = True)\n",
    "      link = item['href']\n",
    "      if title and link and \"동영상\" not in title :  # Title이 존재하고 \"동영상\"이 포함되지 않은 경우에만 추가\n",
    "        news_links.add((title, link))\n",
    "    press_data[press_name] = list(news_links)[:5]  # 다시 list로 변환 후 상위 5개만 저장\n",
    "    \n",
    "    # 각 언론사별 뉴스 Crawling 후 대기 시간 추가\n",
    "    time.sleep(random.uniform(0.5, 2.0))\n",
    "  \n",
    "  return press_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "언론사별 뉴스 Crawling: 100%|██████████| 82/82 [01:40<00:00,  1.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Press</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>세계일보</td>\n",
       "      <td>檢, ‘제3자 뇌물 혐의’ 이재명 기소… 李 \"검찰 창작 수준 갈수록 떨어져\"</td>\n",
       "      <td>https://n.news.naver.com/article/022/000394111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>세계일보</td>\n",
       "      <td>“나라 구하다 죽었냐” 이태원 참사 막말 김미나, 의원직 유지할까?</td>\n",
       "      <td>https://n.news.naver.com/article/022/000394104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>세계일보</td>\n",
       "      <td>[단독] 미국 출시도 안된 제품이 ‘미국 1위’? 뷰즈 전자담배 광고에 소비자 혼란</td>\n",
       "      <td>https://n.news.naver.com/article/022/000394111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세계일보</td>\n",
       "      <td>한국 골프장 카트가 세계서 가장 비싼 렌터카? [수민이가 화났어요]</td>\n",
       "      <td>https://n.news.naver.com/article/022/000394114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>세계일보</td>\n",
       "      <td>김건희 여사 “진돗개 닮았다” 말에…국견 선물한 투르크 최고지도자</td>\n",
       "      <td>https://n.news.naver.com/article/022/000394100...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Press                                           Title  \\\n",
       "0  세계일보     檢, ‘제3자 뇌물 혐의’ 이재명 기소… 李 \"검찰 창작 수준 갈수록 떨어져\"   \n",
       "1  세계일보           “나라 구하다 죽었냐” 이태원 참사 막말 김미나, 의원직 유지할까?   \n",
       "2  세계일보  [단독] 미국 출시도 안된 제품이 ‘미국 1위’? 뷰즈 전자담배 광고에 소비자 혼란   \n",
       "3  세계일보           한국 골프장 카트가 세계서 가장 비싼 렌터카? [수민이가 화났어요]   \n",
       "4  세계일보            김건희 여사 “진돗개 닮았다” 말에…국견 선물한 투르크 최고지도자   \n",
       "\n",
       "                                                Link  \n",
       "0  https://n.news.naver.com/article/022/000394111...  \n",
       "1  https://n.news.naver.com/article/022/000394104...  \n",
       "2  https://n.news.naver.com/article/022/000394111...  \n",
       "3  https://n.news.naver.com/article/022/000394114...  \n",
       "4  https://n.news.naver.com/article/022/000394100...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "press_news_data = get_news_links_by_press(base_url)\n",
    "\n",
    "# 뉴스 DataFrame 생성\n",
    "news_list = []\n",
    "for press_name, news_data in press_news_data.items() :\n",
    "  for title, link in news_data :\n",
    "    news_list.append([press_name, title, link])\n",
    "df = pd.DataFrame(news_list, columns = ['Press', 'Title', 'Link'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gwangyeong/Desktop/Development_231120/과제/[240530~240614_Main_Project_05_Python,Swift] HaruSijack_Application/Data 분석/Scrapy Crawling\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오늘 날짜 가져오기\n",
    "today = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "df.to_csv(f\"Data/NAVER_News_List_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 언론사 목록 가져오기\n",
    "# press_list = list(df.Press.unique())  # random.sample()은 Sequence Type의 Data만 지원하기 때문에 df.Press.unique()의 결과를 List로 변환해야 한다.\n",
    "\n",
    "# # 2. 랜덤으로 10개의 언론사 선택\n",
    "# selected_press = random.sample(press_list, 10)\n",
    "\n",
    "# # # 3. 선택된 언론사에서 Random으로 1~5개의 Link 추출\n",
    "# links = []\n",
    "# for press in selected_press :\n",
    "#   links.extend(df[df['Press'] == press].sample(random.randint(1, 5))['Link'].tolist())\n",
    "\n",
    "# links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 본문 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 기본 File Name 설정\n",
    "# base_filename = df[df['Link'] == link]['Title'].values[0]\n",
    "# extension = \".png\"\n",
    "# name_index = 1  # File Name에 추가될 숫자\n",
    "# new_filename = base_filename + extension\n",
    "\n",
    "# # 기본 Path 설정\n",
    "# ## 오늘 날짜 가져오기\n",
    "# today = datetime.now().strftime('%Y%m%d')\n",
    "# base_path = f\"Data/ScreenShot/By_Press/{today}\"\n",
    "# # By_Press 폴더 생성\n",
    "# os.makedirs(base_path, exist_ok = True)\n",
    "# path_index = 1  # Path에 추가될 숫자\n",
    "\n",
    "# # 동일한 By_Press/today 폴더가 존재하는지 확인하고, 존재한다면 하위에 새로운 Folder 생성\n",
    "# while Path(base_path).exists() :\n",
    "#     file_path = base_path + f\"{path_index}\"\n",
    "#     path_index += 1\n",
    "\n",
    "# # File Name\n",
    "# screenshot_Name = file_path + new_filename\n",
    "\n",
    "# # 동일한 File Name이 존재하는지 확인하고, 존재한다면 새로운 File Name 생성\n",
    "# while Path(new_filename).exists() :\n",
    "#     new_filename = f\"{base_filename}_{name_index}{extension}\"\n",
    "#     name_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling 진행 중: 100%|██████████| 30/30 [02:22<00:00,  4.76s/Link]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 1: 연세의대 교수 비대위 12일 결의전의교협도 총회서 집단행동 논의 서울대병원 교수들이 오는 17일부터 무기한 전체휴진을 예고한 가운데 12일 오전 서울 종로구 서울대학교병원에서 한 ...\n",
      "기사 2: 연합뉴스[서울경제] 12일 강릉 아침 최저기온이 26.3도를 기록해 전날 올해 전국 첫 열대야에 이어 이틀 연속 열대야가 발생해 후텁지근하고 견디기 힘든 무더운 밤이 이어졌다.열대...\n",
      "기사 3: 활동적인 푸바오를 위한 맞춤형 공간가장 자연친화적인 선수핑기지서 생활샤오치지·셩이 대비 방사장 크기 작아[서울경제] 중국 판다보호연구센터 워룽선수핑기지 관계자들이 11일 공개한 푸...\n",
      "기사 4: 사진=이미지투데이[서울경제] 세계적으로 통풍 환자의 수가 가파른 증가 추세를 보이고 있는 가운데 환자 연령대도 낮아지고 있다. 우리나라도 예외는 아니다.건강보험심사평가원의 국민관심...\n",
      "기사 5: Video will play after Ad Next subject author Cancel \"진실 밝히고 박정훈 명예회복\"‥아들 순직 1년 만에 '첫 입장' MBC뉴스 Play ...\n",
      "기사 6: Video will play after Ad Next subject author Cancel 헬스장에 '아줌마 출입 금지' 안내문‥이유는? MBC뉴스 Play 9.7K 0:00:0...\n",
      "기사 7: [Food Trend] 건강에 좋은 60가지 유기산 담긴 흑초·누룩 소금 인기 초루에서 판매 중인 자연 발효 흑초. [초루 제공]김치, 된장, 식초, 요구르트의 공통점은 모두 ‘발...\n",
      "기사 8: 시진핑 정권에 좌절한 30, 40대 中 중산층이 불법 이민 감행 지난해 10월 24일(현지 시간) 중국인들이 망명을 위해 멕시코 국경을 넘은 후 미국 캘리포니아주 자쿰바 인근에서 ...\n",
      "기사 9: 기업 기증 새 물품을 시중 가격의 3분의 1에 판매… 수익금은 취약계층 일자리 제공5월 30일 오후 기자가 찾은 서울 중구 기증품 판매점 ‘굿윌스토어’는 평일인데도 사람들로 북적거...\n",
      "기사 10: 최근 가격 상승은 미국서 촉발… 천연가스 가격도 데이터센터 전력 수요가 견인원자재 가격이 고공 행진을 이어가면서 투자자들 관심이 쏠리고 있다. 구리 가격이 t당 1만 달러(약 13...\n",
      "기사 11: 법조계 \"몇 명이든 제3자 고발인은 재판 개입 못해…심리적 압력일 뿐\"반려견 훈련사인 강형욱 부부가 직원들의 사내 메신저 대화를 무단열람했다는 이유로 전 직원들에게 고소당한 가운데...\n",
      "기사 12: 사나운 사자의 심박 수 측정에 애플의 스마트워치 ‘애플워치’가 활용됐다는 소식이 나왔다.IT매체 애플인사이더는 11일(현지시간) 호주의 한 수의사가 진정제를 투여한 사자의 혀에 애...\n",
      "기사 13: [한반도 지오그래픽] 北 복합 도발은 초조함 발로● 김여정 ‘대북전단=오물’은 가계도 비하한 셈 ● 서해 GPS 교란 공격은 회색지대 전술● 위력시위사격, KN-25 ‘연사 능력 ...\n",
      "기사 14: ⓒ유튜브 캡처[데일리안 = 이지희 기자] 일부 유튜버들이 '밀양 여중생 성폭행 사건' 가해자들의 신상을 잇따라 폭로해 사적제재 논란이 일고 있는 가운데 여섯 번째 가해자로 지목된 ...\n",
      "기사 15: 예천앙조, 2020년 영탁과 1년 계약 맺고 '영탁막걸리' 출시이듬해 재계약 결렬…영탁 측, 상표 사용금지 청구소송 제기1·2심 영탁 승소…예천양조, 상고이유서 미제출로 판결 확정...\n",
      "기사 16: 더스쿠프 마켓톡톡민주당 2호법안 상법 개정안서 이사의 회사 충실의무에 주주 포함 쪼개기 상장, 일감 몰아주기 등재벌 총수 일가 승계 수단과 직결이익단체들, 소송 급증 이유로 반대회...\n",
      "기사 17: 더스쿠프 심층취재 추적+택배업체 속도경쟁 속 과로사 엔데믹 국면에서 문제 불거져택배 과로방지 대책 발표 3년 後분류 인력 배치·주 60시간 근무 분류 비용 착복 대리점 적지 않아사...\n",
      "기사 18: 더스쿠프 이슈 아카이브 소비자물가지수 분석해보니 4년 전보다 14.1% 치솟아 458개 품목 중 안 오른 건 37개 가공식품‧외식 모조리 다 올라 물가의 난… 서민 고통 커져 소비...\n",
      "기사 19: 더스쿠프 마켓분석 자체 생산한 전기차 3종 공개아이폰처럼 전기차도 OEM화800여개 기업과 컨소시엄 완료'전기차 파운드리'로 미래 바꿀까미래 모빌리티 시장의 리더가 되기 위한 기업...\n",
      "기사 20: 세브란스병원 교수들이 지난 4월 30일 하루 외래 진료와 수술을 중단하고 피케팅을 하고 있다. 사진=뉴시스 서울대의대·서울대병원 교수들이 집단휴진을 선언한 데 이어 연세대의대 산하...\n",
      "기사 21: 지난해 7월 20일 경북 포항시 해병대 1사단에 마련된 고 채수근 상병 빈소에서 채 상병의 어머니가 오열하고 있다. 사진=뉴시스 해병대 고(故) 채 상병 어머니가 채상병 순직 1주...\n",
      "기사 22: [사진=뉴시스] 서울 신림동 성폭행 살인범 최윤종에게 2심에서도 무기징역이 선고됐습니다. 서울고등법원은 오늘(12일) 성폭력처벌법 위반 등의 혐의로 구속 기소된 최윤종에 1심과 같...\n",
      "기사 23: 골프선수 출신 박세리씨가 이끄는 박세리희망재단이 박씨의 부친을 사문서위조 혐의로 경찰에 고소했다. /박세리희망재단 홈페이지 골프선수 출신 박세리씨가 이끄는 박세리희망재단이 박씨의 ...\n",
      "기사 24: 강아지를 유기하고 떠난 것으로 추정되는 차량./보배드림 가족 여행을 하던 중 강아지를 유기하는 장면을 목격했다는 주장이 제기됐다.지난 10일 한 온라인 커뮤니티에 ‘강아지 유기를 ...\n",
      "기사 25: 나경원 국민의힘 의원과 한동훈 전 비상대책위원장. /뉴스1 나경원 국민의힘 의원이 한동훈 전 비상대책위원장이 이재명 더불어민주당 대표의 ‘사법리스크’를 거론한 것과 관련해 “이 대...\n",
      "기사 26: 유튜버, 가해자 거주 아파트 명칭 공개동·호수까지 노출… 입주민 불안 확산아파트 측 \"입대의, 경찰 도움 요청\" 한 유튜버가 지난 9일 자신의 유튜브 채널에서 밀양 집단 성폭행 사...\n",
      "기사 27: 지린성 미국인 피습 사건 용의자 50대 남성 체포현지 공안 \"미국인들과 공원서 부딪힌 뒤 칼부림\"중국 온라인서 \"가해자도 이유 있어서 그랬을 것\" 10일 중국 지린성에 위치한 베이...\n",
      "기사 28: 가해 중대장이 구급차 선탑자 역할민간병원 기록엔 가혹행위 내용 무첫 치료한 의무대엔 아예 기록 없어 임태훈 군인권센터 소장이 4일 서울 용산구 국방부 앞에서 열린 '육군 12사단 ...\n",
      "기사 29: [건강이 최고] 강박장애, 20~30대 젊은 환자가 가장 많아 게티이미지뱅크반복적이고 지속적인 생각이나 행동으로 고통을 겪는 상태를 '강박장애'라고 한다. 강박장애가 일상생활에 지...\n",
      "기사 30: 전북·경기·충남 등 유감 신고 322건벽면 금 가는 등 시설 피해 잇따라올해 최대 규모, 인명 피해 없어부안서 규모 3.1 추가 지진 발생 12일 오전 전북 부안군에서 발생한 4....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwangyeong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwangyeong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Text 처리 중: 100%|██████████| 30/30 [00:01<00:00, 24.23기사/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 1 키워드: ['수', '구독', '닫기', '교수', '무기한', '휴진을', '응답은', '휴진', '있습니다', '언론사']\n",
      "기사 2 키워드: ['구독', '닫기', '수', '기자', '있습니다', '언론사', '서울경제', '시원한', '기사를', '볼']\n",
      "기사 3 키워드: ['수', '푸바오의', '푸바오가', '구독', '푸바오는', '닫기', '실외', '볼', '방사장', '지낼']\n",
      "기사 4 키워드: ['수', '구독', '통풍', '닫기', '많이', '기자', '있습니다', '언론사', '서울경제', '있는']\n",
      "기사 5 키워드: ['며', '구독', '수', '박정훈', 'subtitle', '아들의', '고', '있습니다', '해병대', '어머니는']\n",
      "기사 6 키워드: ['subtitle', '수', '있습니다', '언론사', 'play', '480p', '한', '노', '기사를', '닫기']\n",
      "기사 7 키워드: ['자연', '발효', '수', '건강에', '소금', '0', '주간동아', '있습니다', '언론사', '과정을']\n",
      "기사 8 키워드: ['미국', '불법', '미국으로', '중국인', '중국', '지난해', '중국인은', '최근', '망명', '파나마']\n",
      "기사 9 키워드: ['기증품', '수', '판매점', '새', '굿윌스토어', '는', '있는', '고', '있습니다', '한']\n",
      "기사 10 키워드: ['금', '구리', '천연가스', '가격', '가격은', '올해', '최근', '약', '이후', '미국']\n",
      "기사 11 키워드: ['수', '구독', '강형욱', '고발인은', '사내', '메신저', '법적', '것으로', '고', '없다']\n",
      "기사 12 키워드: ['수', '구독', '심박', '닫기', '사자의', '애플워치', '기자', '있습니다', '언론사', '애플워치를']\n",
      "기사 13 키워드: ['김정은', '6월', '오물', '수', '교란', '5월', '북한이', 'gps', '북한은', '북한']\n",
      "기사 14 키워드: ['고', '구독', '수', '닫기', '기자', '밀양', '해당', 'a씨는', '있습니다', '언론사']\n",
      "기사 15 키워드: ['닫기', '구독', '수', '기자', '없이', '영탁', '예천양조는', '디케의', '눈물', '있습니다']\n",
      "기사 16 키워드: ['수', '상법', '재벌', '구독', '더스쿠프', '닫기', '있습니다', '주주', '총수', '게']\n",
      "기사 17 키워드: ['사회적', '택배', '구독', '과로방지', '택배노동자', '분류작업', '과로사', '2021년', '택배노동자들의', '닫기']\n",
      "기사 18 키워드: ['구독', '대비', '2020년', '품목', '가격이', '품목은', '수', '기자', '닫기', '있습니다']\n",
      "기사 19 키워드: ['전기차', '전기차를', '수', '구독', '폭스콘이', '건', '파운드리', '미래', '시장의', '닫기']\n",
      "기사 20 키워드: ['있습니다', '언론사', '고', '채널a', '기사를', '수', '닫기', '구독', '세브란스병원', '교수들이']\n",
      "기사 21 키워드: ['해병대', '고', '상병', '있습니다', '언론사', '7월', '채널a', '기사를', '수', '닫기']\n",
      "기사 22 키워드: ['구독', '수', '기자', '닫기', '있습니다', '언론사', '최윤종에게', '고', '채널a', '기사를']\n",
      "기사 23 키워드: ['수', '구독', '닫기', '며', '기자', '있습니다', '언론사', '보기', '출신', '박씨의']\n",
      "기사 24 키워드: ['수', '구독', '한', '닫기', '강아지', '있었다', '고', '기자', '있습니다', '언론사']\n",
      "기사 25 키워드: ['수', '구독', '전', '닫기', '보기', '이재명의', '민주당', '고', '이미', '며']\n",
      "기사 26 키워드: ['아파트', '한', '닫기', '구독', '수', '밀양', '해당', '기자', '출입', '있습니다']\n",
      "기사 27 키워드: ['중국', '고', '구독', '사건이', '수', '닫기', '미국인', '것으로', '지린성', '미국']\n",
      "기사 28 키워드: ['닫기', '구독', '훈련병', '고', '가혹행위', '사건', '있습니다', '수', '군인권센터', '사망']\n",
      "기사 29 키워드: ['구독', '한다', '수', '닫기', '특정', '반복적으로', '유형', '물건을', '강박', '생각에']\n",
      "기사 30 키워드: ['기자', '구독', '규모', '고', '며', '발생한', '한', '부안군', '수', '닫기']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Chrome Browser와 Chrome Driver Version 확인 및 WebDriver 객체 생성\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "# ******************************************************\n",
    "chrome_options.add_argument('headless') # Run chrome browser in the background\n",
    "chrome_options.add_argument('window-size = 1920x1080')  # Chrome Browser Window Size\n",
    "# ******************************************************\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options = chrome_options)\n",
    "\n",
    "# 1. 언론사 목록 가져오기\n",
    "press_list = list(df.Press.unique())  # random.sample()은 Sequence Type의 Data만 지원하기 때문에 df.Press.unique()의 결과를 List로 변환해야 한다.\n",
    "\n",
    "# 2. 랜덤으로 10개의 언론사 선택\n",
    "selected_press = random.sample(press_list, 10)\n",
    "\n",
    "# # 3. 선택된 언론사에서 Random으로 1~5개의 Link 추출\n",
    "links = []\n",
    "for press in selected_press :\n",
    "    links.extend(df[df['Press'] == press].sample(random.randint(1, 5))['Link'].tolist())\n",
    "\n",
    "# tqdm Module을 이용하여 진행 상황 표시\n",
    "pbar = tqdm(links, desc = \"Crawling 진행 중\", unit = \"Link\")\n",
    "\n",
    "# 뉴스 기사 Link를 순회하며 기사 내용 추출\n",
    "articles = []\n",
    "for link in pbar :\n",
    "    driver.get(link)\n",
    "\n",
    "    # **************************************************************************\n",
    "    # 기본 Path 설정\n",
    "    today = datetime.now().strftime('%Y%m%d')   # 오늘 날짜 가져오기\n",
    "    base_path = f\"Data/ScreenShot/By_Press/{today}\"\n",
    "\n",
    "    ## By_Press 폴더 생성\n",
    "    os.makedirs(base_path, exist_ok = True)\n",
    "\n",
    "    # 동일한 By_Press/today Folder가 존재하는지 확인하고, 존재한다면 하위에 새로운 Folder 생성\n",
    "    new_path = base_path\n",
    "    while os.path.exists(new_path) :\n",
    "        new_path = os.path.join(base_path, str(len(os.listdir(base_path)) + 1)) # index는 폴더 내 파일 개수 + 1로 설정\n",
    "    \n",
    "    # 새로운 Folder 생성\n",
    "    os.makedirs(new_path, exist_ok = True)\n",
    "\n",
    "    # 기본 File Name 설정\n",
    "    base_filename = df[df['Link'] == link]['Title'].values[0]\n",
    "    extension = \".png\"\n",
    "    name_index = 1  # File Name에 추가될 숫자\n",
    "    new_filename = base_filename + extension\n",
    "\n",
    "    # File Path 생성\n",
    "    screenshot_name = os.path.join(new_path, new_filename)\n",
    "\n",
    "    # 동일한 File Name이 존재하는지 확인하고, 존재한다면 새로운 File Name 생성\n",
    "    while os.path.exists(screenshot_name) :\n",
    "        new_filename = f\"{base_filename}_{name_index}{extension}\"\n",
    "        screenshot_name = os.path.join(new_path, new_filename)\n",
    "        name_index += 1\n",
    "\n",
    "    # ScreenShot 촬영 전에 시간 두기. (Loading이 느릴수도 있으므로...)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Browser 최대화\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    # 현재 화면 Capture하기\n",
    "    driver.save_screenshot(screenshot_name)\n",
    "    # **************************************************************************\n",
    "\n",
    "    html = driver.page_source\n",
    "    article_soup = BeautifulSoup(html, \"html.parser\")\n",
    "    content = article_soup.select_one(\"#contents\")\n",
    "    if content :\n",
    "        # 공백과 HTML Tag 제거\n",
    "        text = ' '.join(content.text.split())\n",
    "        articles.append(text)\n",
    "    # 요청 후 임의의 시간만큼 대기 (Page Loaded)\n",
    "    time.sleep(random.uniform(0.5, 2.0))\n",
    "\n",
    "# Browser 종료 (모든 Tab 종료)\n",
    "driver.quit()\n",
    "\n",
    "# 빈 문서가 있는지 확인\n",
    "articles = [article for article in articles if article.strip()]\n",
    "\n",
    "# 기사 내용이 제대로 수집되었는지 확인\n",
    "for i, article in enumerate(articles) :\n",
    "    print(f\"기사 {i + 1}: {article[:100]}...\")  # 기사 내용 앞부분만 출력\n",
    "\n",
    "# NLTK를 이용하여 불용어 제거, 단어 토큰화, 표제어 추출\n",
    "## 한국어 불용어 모음집 불러오기\n",
    "stopword_list = pd.read_csv(\"Data/stopword.txt\", header = None)\n",
    "stopword_list[0] = stopword_list[0].apply(lambda x: x.strip())\n",
    "korean_stopwords = stopword_list[0].to_numpy()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_articles = []\n",
    "for article in tqdm(articles, desc = \"Text 처리 중\", unit = \"기사\") :\n",
    "    tokens = word_tokenize(article)\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalnum() and word.lower() not in korean_stopwords]\n",
    "    tokenized_articles.append(' '.join(tokens))\n",
    "\n",
    "# 빈 문서가 있는지 다시 확인\n",
    "tokenized_articles = [article for article in tokenized_articles if article.strip()]\n",
    "\n",
    "# 각 기사별 Keyword 추출 (빈도 높은 단어)\n",
    "keywords = []\n",
    "for article in tokenized_articles :\n",
    "    word_counts = Counter(article.split())\n",
    "    common_words = word_counts.most_common(10)  # 상위 10개 단어 추출\n",
    "    keywords.append([word for word, freq in common_words])\n",
    "\n",
    "# Keyword 확인\n",
    "for i, kw in enumerate(keywords) :\n",
    "    print(f\"기사 {i + 1} 키워드: {kw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> base_path 설정 부분: <br>\n",
    "os.path.exists(file_path) 조건을 사용하여 실제 경로가 존재하는지 확인합니다. <br>\n",
    "경로가 존재하면 file_path에 새로운 폴더 번호를 추가하여 경로를 생성합니다.\n",
    "\n",
    "> new_filename 생성 부분: <br>\n",
    "os.path.exists(screenshot_name) 조건을 사용하여 실제 파일 경로가 존재하는지 확인합니다. <br>\n",
    "파일이 존재하면 new_filename에 새로운 번호를 추가하여 파일 이름을 생성합니다.\n",
    "\n",
    "> os.path.join() 함수는 운영 체제에 맞는 경로 구분자(Windows의 '', Unix/Linux의 '/')를 자동으로 처리해주기 때문에, <br>\n",
    "상대경로나 절대경로를 모두 안전하게 연결할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chrome Browser와 Chrome Driver Version 확인\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('headless')  # Run chrome browser in the background\n",
    "# chrome_options.add_argument('window-size=1920x1080')  # Chrome Browser Window Size\n",
    "\n",
    "# \"\"\"\n",
    "#     본문 수집 함수 작성:\n",
    "#     - 주어진 Link에서 기사 본문을 수집하고 ScreenShot을 저장합니다.\n",
    "#     - ScreenShot 저장 경로 설정, File Name 중복 처리 등 추가 기능을 포함합니다.\n",
    "# \"\"\"\n",
    "# def fetch_article_content (link) :\n",
    "#     \"\"\"\n",
    "#     주어진 URL에서 기사의 본문을 수집하는 함수\n",
    "#     \"\"\"\n",
    "#     try :\n",
    "#         driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options = chrome_options)\n",
    "#         driver.get(link)\n",
    "\n",
    "#         # ************************************************************************************************\n",
    "#         # 기본 Path 설정\n",
    "#         today = datetime.now().strftime('%Y%m%d')\n",
    "#         base_path = f\"../Data/ScreenShot/By_Press/{today}\"\n",
    "#         os.makedirs(base_path, exist_ok = True)\n",
    "\n",
    "#         # 동일한 By_Press/today Folder가 존재하는지 확인하고, 존재한다면 하위에 새로운 Folder 생성\n",
    "#         new_path = base_path\n",
    "#         while os.path.exists(new_path) :\n",
    "#             new_path = os.path.join(base_path, str(len(os.listdir(base_path)) + 1)) # index는 Folder 내 File 갯수 + 1로 설정\n",
    "\n",
    "#         # 새로운 Folder 생성\n",
    "#         os.makedirs(new_path, exist_ok = True)\n",
    "\n",
    "#         # 기본 File Name 설정\n",
    "#         base_filename = df[df['Link'] == link]['Title'].values[0]\n",
    "#         extension = \".png\"\n",
    "#         name_index = 1\n",
    "#         new_filename = base_filename + extension\n",
    "\n",
    "#         # File Path 생성\n",
    "#         screenshot_name = os.path.join(new_path, new_filename)\n",
    "\n",
    "#         # 동일한 File Name이 존재하는지 확인하고, 존재한다면 새로운 File Name 생성\n",
    "#         while os.path.exists(screenshot_name) :\n",
    "#             new_filename = f\"{base_filename}_{name_index}{extension}\"\n",
    "#             screenshot_name = os.path.join(new_path, new_filename)\n",
    "#             name_index += 1\n",
    "\n",
    "#         # ScreenShot 촬영 전에 시간 두기. (Loading이 느릴수도 있으므로...)\n",
    "#         time.sleep(3)\n",
    "        \n",
    "#         # Browser 최대화\n",
    "#         driver.maximize_window()\n",
    "        \n",
    "#         # 현재 화면 Capture하기\n",
    "#         driver.save_screenshot(screenshot_name)\n",
    "#         # ************************************************************************************************\n",
    "\n",
    "#         html = driver.page_source\n",
    "#         driver.quit()  # Browser 종료 (모든 Tab 종료)\n",
    "#         article_soup = BeautifulSoup(html, \"html.parser\")\n",
    "#         content = article_soup.select_one(\"#contents\")\n",
    "#         if content :\n",
    "#             # 공백과 HTML Tag 제거\n",
    "#             text = ' '.join(content.text.split())\n",
    "#             return text\n",
    "#         else :\n",
    "#             return None\n",
    "        \n",
    "#     except Exception as e :\n",
    "#         print(f\"Error fetching {link}: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "#     Multiprocessing을 사용하여 본문 수집:\n",
    "#     - 모든 Link를 가져와 List로 변환합니다.\n",
    "#     - Pool(processes=4): 4개의 Process를 가진 Pool을 생성합니다.\n",
    "#     - pool.imap(fetch_article_content, links): Link List를 동시에 처리합니다.\n",
    "# \"\"\"\n",
    "# # 모든 Link 가져오기\n",
    "# links = df['Link'].tolist()\n",
    "\n",
    "# # Multi Processing을 사용하여 본문 수집\n",
    "# pool = Pool(processes = 4)  # 4개의 Process를 가진 Pool을 생성\n",
    "# results = list(\n",
    "#     tqdm(\n",
    "#         pool.imap(fetch_article_content, links),\n",
    "#         total = len(links),\n",
    "#         desc = \"Crawling 진행 중\",\n",
    "#         unit = \"Link\"\n",
    "#         )\n",
    "# )\n",
    "# pool.close()  # 작업이 끝난 후 Pool을 닫음\n",
    "# pool.join()  # 모든 Process가 끝날 때까지 기다림\n",
    "\n",
    "# # 빈 문서가 있는지 확인\n",
    "# articles = [article for article in results if article and article.strip()]\n",
    "\n",
    "# # 기사 내용이 제대로 수집되었는지 확인\n",
    "# for i, article in enumerate(articles) :\n",
    "#     print(f\"기사 {i + 1}: {article[:100]}...\")  # 기사 내용 앞부분만 출력\n",
    "\n",
    "# # NLTK를 이용하여 불용어 제거, 단어 토큰화, 표제어 추출\n",
    "# stopword_list = pd.read_csv(\"../Data/stopword.txt\", header = None)\n",
    "# stopword_list[0] = stopword_list[0].apply(lambda x: x.strip())\n",
    "# korean_stopwords = stopword_list[0].to_numpy()\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# tokenized_articles = []\n",
    "# for article in tqdm(articles, desc = \"Text 처리 중\", unit = \"기사\"):\n",
    "#     tokens = word_tokenize(article)\n",
    "#     tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalnum() and word.lower() not in korean_stopwords]\n",
    "#     tokenized_articles.append(' '.join(tokens))\n",
    "\n",
    "# # 빈 문서가 있는지 다시 확인\n",
    "# tokenized_articles = [article for article in tokenized_articles if article.strip()]\n",
    "\n",
    "# # 각 기사별 Keyword 추출 (빈도 높은 단어)\n",
    "# keywords = []\n",
    "# for article in tokenized_articles :\n",
    "#     word_counts = Counter(article.split())\n",
    "#     common_words = word_counts.most_common(10)  # 상위 10개 단어 추출\n",
    "#     keywords.append([word for word, freq in common_words])\n",
    "\n",
    "# # Keyword 확인\n",
    "# for i, kw in enumerate(keywords) :\n",
    "#     print(f\"기사 {i + 1} 키워드: {kw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
