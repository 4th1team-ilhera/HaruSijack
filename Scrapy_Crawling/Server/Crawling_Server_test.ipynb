{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###  crawling server test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/forrestdpark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Library Package\n",
    "## Flask Server\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "## Basic\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "## Crawling\n",
    "import requests # 인터넷에서 Data를 가져오기 위한 Library (웹페이지에 접속하고 HTML 코드를 가져오기 위해 사용)\n",
    "from bs4 import BeautifulSoup # 웹 페이지 내용을 분석하기 위한 Library (가져온 HTML 코드에서 우리가 필요한 정보를 추출하기 위해 사용)\n",
    "\n",
    "import time # 대기 시간을 추가하기 위한 Library (요청 사이에 랜덤한 시간을 기다리기 위해 사용)\n",
    "import random # Random한 대기 시간을 만들기 위한 Library\n",
    "from tqdm import tqdm # Crawling 진행 상황을 체크하기 위한 Module (진행 상황을 시각적으로 보여주기 위해 사용)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "## Bag of Words (BoW)\n",
    "import nltk # Natural Language Toolkit (자연어 처리를 위해 사용)\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter # 단어의 빈도를 계산하기 위해 사용\n",
    "\n",
    "### 1. nltk Data Download\n",
    "nltk.download('punkt')\n",
    "\n",
    "### 2. 한국어 불용어 사전\n",
    "# *************************************************************************\n",
    "## 한국어 불용어 모음집 불러오기\n",
    "stopword_list = pd.read_csv(\"../Data/updated_stopword.txt\", header = None)\n",
    "# *************************************************************************\n",
    "stopword_list[0] = stopword_list[0].apply(lambda x: x.strip())\n",
    "stopwords = stopword_list[0].to_numpy()\n",
    "\n",
    "## Deep Learning\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## app 에서 받은 키워드 \n",
    "keyword1 = \"자동차\"\n",
    "keyword2 = \"반도체\"\n",
    "keyword3 = \"IT\"\n",
    "\n",
    "#  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국일보\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경향신문\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중앙일보\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동아일보\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTBC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조선일보\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국경제\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YTN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연합뉴스\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "언론사별 뉴스 Crawling: 100%|██████████| 82/82 [00:05<00:00, 15.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. NAVER News Web Crawling\n",
    "## 1-1. 언론사별 랭킹뉴스 Crawling 함수 정의\n",
    "def get_news_links_by_press (url,artilceNum) :\n",
    "  \"\"\"\n",
    "    headers:\n",
    "    - 나는 bot이 아니고 사람임을 증명하는 부분이다.\n",
    "    - 사용하지 않을 시 언론사에서 웹크롤링을 막을 수 있으니 주의할 것!\n",
    "  \"\"\"\n",
    "  headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "  }\n",
    "  response = requests.get(url, headers = headers) # url(페이지)에 접속\n",
    "  soup = BeautifulSoup(response.content, 'html.parser') # HTML 코드를 파싱(분석)하여 soup 객체에 저장\n",
    "  \n",
    "  press_data = {}\n",
    "  # naver ranking news  \n",
    "  press_sections = soup.select('.rankingnews_box')\n",
    "  press_list = [\n",
    "      \"중앙일보\",\n",
    "      \"연합뉴스\",\n",
    "      \"경향신문\",\n",
    "      \"조선일보\",\n",
    "      \"JTBC\",\n",
    "      \"한국일보\",\n",
    "      \"한국경제\",\n",
    "      \"MBC\",\n",
    "      \"YTN\",\n",
    "      \"동아일보\"                          \n",
    "    ]\n",
    "  for press_section in tqdm(press_sections, desc = \"언론사별 뉴스 Crawling\") : # tqdm : 크롤링 시각화 \n",
    "    press_name = press_section.select_one('.rankingnews_name').get_text(strip = True)\n",
    "    # print(press_name)\n",
    "    if press_name in press_list:\n",
    "      print(press_name)\n",
    "      news_links = set()  # 중복 제거를 위한 set 사용\n",
    "      for item in press_section.select('li a') :\n",
    "        title = item.get_text(strip = True)\n",
    "        link = item['href']\n",
    "        if title and link and \"동영상\" not in title :  # Title이 존재하고 \"동영상\"이 포함되지 않은 경우에만 추가\n",
    "          news_links.add((title, link))\n",
    "      press_data[press_name] = list(news_links)[:artilceNum]  # 다시 list로 변환 후 상위 5개만 저장\n",
    "      # *********************************************\n",
    "      # 각 언론사별 뉴스 Crawling 후 대기 시간 추가\n",
    "      time.sleep(random.uniform(0.32, 0.8))\n",
    "      # *********************************************\n",
    "  return press_data\n",
    "\n",
    "## 1-2. 언론사별 랭킹뉴스의 Press, Title, Link를 토대로 하는 DataFrame 생성\n",
    "base_url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "press_news_data = get_news_links_by_press(base_url,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(press_news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "언론사별 뉴스 Crawling: 100%|██████████| 83/83 [01:01<00:00,  1.36it/s]\n",
      "뉴스 기사 본문 Crawling 진행 중:  48%|████▊     | 194/407 [03:03<03:13,  1.10Link/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m## 2-3. 뉴스 기사 본문 수집\u001b[39;00m\n\u001b[1;32m     72\u001b[0m content_pbar \u001b[38;5;241m=\u001b[39m tqdm(news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m'\u001b[39m], desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m뉴스 기사 본문 Crawling 진행 중\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [get_article_text(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m content_pbar]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m## 2-4. Browser 종료 (모든 Tab 종료)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m## 2-3. 뉴스 기사 본문 수집\u001b[39;00m\n\u001b[1;32m     72\u001b[0m content_pbar \u001b[38;5;241m=\u001b[39m tqdm(news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m'\u001b[39m], desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m뉴스 기사 본문 Crawling 진행 중\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [get_article_text(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m content_pbar]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m## 2-4. Browser 종료 (모든 Tab 종료)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mget_article_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     63\u001b[0m text \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(content\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# *******************************************\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 요청 후 임의의 시간만큼 대기 (Page Loaded)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.8\u001b[39m))\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# *******************************************\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. NAVER News Web Crawling\n",
    "## 1-1. 언론사별 랭킹뉴스 Crawling 함수 정의\n",
    "def get_news_links_by_press (url) :\n",
    "  \"\"\"\n",
    "    headers:\n",
    "    - 나는 bot이 아니고 사람임을 증명하는 부분이다.\n",
    "    - 사용하지 않을 시 언론사에서 웹크롤링을 막을 수 있으니 주의할 것!\n",
    "  \"\"\"\n",
    "  headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "  }\n",
    "  response = requests.get(url, headers = headers) # url(페이지)에 접속\n",
    "  soup = BeautifulSoup(response.content, 'html.parser') # HTML 코드를 파싱(분석)하여 soup 객체에 저장\n",
    "  \n",
    "  press_data = {}\n",
    "  # naver ranking news  \n",
    "  press_sections = soup.select('.rankingnews_box')\n",
    "  \n",
    "  for press_section in tqdm(press_sections, desc = \"언론사별 뉴스 Crawling\") : # tqdm : 크롤링 시각화 \n",
    "    press_name = press_section.select_one('.rankingnews_name').get_text(strip = True)\n",
    "    news_links = set()  # 중복 제거를 위한 set 사용\n",
    "    for item in press_section.select('li a') :\n",
    "      title = item.get_text(strip = True)\n",
    "      link = item['href']\n",
    "      if title and link and \"동영상\" not in title :  # Title이 존재하고 \"동영상\"이 포함되지 않은 경우에만 추가\n",
    "        news_links.add((title, link))\n",
    "    press_data[press_name] = list(news_links)[:5]  # 다시 list로 변환 후 상위 5개만 저장\n",
    "    # *********************************************\n",
    "    # 각 언론사별 뉴스 Crawling 후 대기 시간 추가\n",
    "    time.sleep(random.uniform(0.5, 1.0))\n",
    "    # *********************************************\n",
    "  \n",
    "  return press_data\n",
    "\n",
    "## 1-2. 언론사별 랭킹뉴스의 Press, Title, Link를 토대로 하는 DataFrame 생성\n",
    "base_url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "press_news_data = get_news_links_by_press(base_url)\n",
    "\n",
    "news_list = []\n",
    "for press_name, news_data in press_news_data.items() :\n",
    "  for title, link in news_data :\n",
    "    news_list.append([press_name, title, link])\n",
    "news_df = pd.DataFrame(news_list, columns = ['Press', 'Title', 'Link'])\n",
    "\n",
    "\n",
    "# 2. 언론사별 뉴스 기사 본문 Crawling\n",
    "## 2-1. Chrome Browser와 Chrome Driver Version 확인 및 WebDriver 객체 생성\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "# ******************************************************\n",
    "chrome_options.add_argument('headless') # Run chrome browser in the background\n",
    "chrome_options.add_argument('window-size = 1920x1080')  # Chrome Browser Window Size\n",
    "# ******************************************************\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options = chrome_options)\n",
    "\n",
    "# 2-2. 언론사별 뉴스 기사 본문 Crawling 함수 정의\n",
    "def get_article_text (url) :\n",
    "  driver.get(url)\n",
    "  html = driver.page_source\n",
    "  article_soup = BeautifulSoup(html, \"html.parser\")\n",
    "  content = article_soup.select_one(\"#contents\")\n",
    "  if content :\n",
    "    # 공백과 HTML Tag 제거\n",
    "    text =''.join(content.text.split())\n",
    "    # *******************************************\n",
    "    # 요청 후 임의의 시간만큼 대기 (Page Loaded)\n",
    "    time.sleep(random.uniform(0.5, 0.8))\n",
    "    # *******************************************\n",
    "    \n",
    "    return text\n",
    "\n",
    "## 2-3. 뉴스 기사 본문 수집\n",
    "content_pbar = tqdm(news_df['Link'], desc = \"뉴스 기사 본문 Crawling 진행 중\", unit = \"Link\")\n",
    "news_df['content'] = [get_article_text(url) for url in content_pbar]\n",
    "\n",
    "## 2-4. Browser 종료 (모든 Tab 종료)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
