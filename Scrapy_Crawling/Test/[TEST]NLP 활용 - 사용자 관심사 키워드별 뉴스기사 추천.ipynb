{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [NLP 활용] 언론사별 상위 10개 뉴스기사 요약본 안내"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_keras\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tf_keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf_keras) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.1.0)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf_keras\n",
      "Successfully installed tf_keras-2.16.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "!pip install tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f3311622eb46f28ccfea8cf857e0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 11:17:33.994085: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-06-12 11:17:33.994112: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-12 11:17:33.994122: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-12 11:17:33.994343: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-12 11:17:33.994355: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "언론사별 뉴스 크롤링: 100%|██████████| 82/82 [01:42<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘의 주요 키워드:\n",
      "지진: 62\n",
      "부안: 45\n",
      "전북: 41\n",
      "규모: 39\n",
      "속보: 17\n",
      "최대: 16\n",
      "올해: 15\n",
      "물풍선: 14\n",
      "\n",
      "추천 뉴스:\n",
      "1. \"싼값에 한 봉지 가득 담아간다\" 하루 2000개씩 팔리는 '1000원 빵'[르포] (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/277/0005430436?ntype=RANKING\n",
      "2. \"침대가 흔들려 깼어\"…부안 지진에 누리꾼들도 화들짝 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/277/0005430575?ntype=RANKING\n",
      "3. \"제발 살아달라\" 필사적으로 매달린 여고생…소중한 생명 구했다 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/277/0005430037?ntype=RANKING\n",
      "4. \"운동하면서 용돈도 벌어요\"…'갓생' 사는 80세 노신사[배달의청춘]② (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/277/0005430443?ntype=RANKING\n",
      "5. 밀양 사건 폭로 유튜버, 생사람 잡았다…6번째 지목男 \"난 가해자 아냐\" (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/277/0005430389?ntype=RANKING\n",
      "6. [단독]\"月300만원 이상도 번다\" 서울 구직 시니어 1만명 돌파 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/025/0003366286?ntype=RANKING\n",
      "7. 고준희, 버닝썬 루머에 \"그 쌍X의 새X들…소속사도 방치하더라\" (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/025/0003366274?ntype=RANKING\n",
      "8. \"창문 깨졌다\" \"벽 갈라져\"…부안 지진, 경기까지 흔들렸다 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/025/0003366310?ntype=RANKING\n",
      "9. 헬로비너스 출신 유아라, 암 투병 고백 \"긴급 수술 후 회복 중\" (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/025/0003366308?ntype=RANKING\n",
      "10. \"금액 어마어마\"…'수원의 딸' 카리나가 부산서 시구한 이유 (POSITIVE)\n",
      "   Link: https://n.news.naver.com/article/025/0003366297?ntype=RANKING\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Server에 부하를 주지 않기 위한 Crawling 속도 제한용 Library Package\n",
    "import time\n",
    "import random\n",
    "# Crawling 진행 상황을 체크하기 위한 Module\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from collections import Counter\n",
    "\n",
    "# 뉴스 Crawling\n",
    "def get_news_links_by_press (url) :\n",
    "  headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "  response = requests.get(url, headers = headers)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  \n",
    "  press_data = {}\n",
    "  press_sections = soup.select('.rankingnews_box')\n",
    "  \n",
    "  for press_section in tqdm(press_sections, desc = \"언론사별 뉴스 크롤링\") :\n",
    "    press_name = press_section.select_one('.rankingnews_name').get_text(strip = True)\n",
    "    news_links = set()  # 중복 제거를 위한 set 사용\n",
    "    for item in press_section.select('li a') :\n",
    "      title = item.get_text(strip = True)\n",
    "      link = item['href']\n",
    "      if title and link and \"동영상\" not in title :  # 타이틀이 존재하고 \"동영상\"이 포함되지 않은 경우에만 추가\n",
    "        news_links.add((title, link))\n",
    "    press_data[press_name] = list(news_links)[:5]  # 다시 list로 변환 후 상위 5개만 저장\n",
    "    \n",
    "    # 각 언론사별 뉴스 Crawling 후 대기 시간 추가\n",
    "    time.sleep(random.uniform(0.5, 2.0))\n",
    "  \n",
    "  return press_data\n",
    "\n",
    "# Data 전처리\n",
    "def preprocess_text (text) :\n",
    "  okt = Okt()\n",
    "  # tokens = okt.morphs(text, stem = True)\n",
    "  tokens = okt.nouns(text)  # 명사만 추출\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# 감성 분석 모델과 토크나이저를 미리 다운로드\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 감성 분석 파이프라인 초기화\n",
    "classifier = pipeline('sentiment-analysis', model = model, tokenizer = tokenizer)\n",
    "\n",
    "# 감성 분석 함수\n",
    "def sentiment_analysis (text) :\n",
    "  return classifier(text)\n",
    "\n",
    "# Topic 모델링\n",
    "def topic_modeling(docs, num_topics = 5) :\n",
    "  vectorizer = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')\n",
    "  dtm = vectorizer.fit_transform(docs)\n",
    "  lda = LatentDirichletAllocation(n_components = num_topics, random_state = 0)\n",
    "  lda.fit(dtm)\n",
    "  return lda, vectorizer\n",
    "\n",
    "# 주요 Keyword 추출\n",
    "# def extract_keywords (docs, num_keywords = 10) :\n",
    "#   words = ' '.join(docs).split()\n",
    "#   counter = Counter(words)\n",
    "#   return counter.most_common(num_keywords)\n",
    "def extract_keywords(docs, num_keywords = 10) :\n",
    "  words = ' '.join(docs).split()\n",
    "  counter = Counter(words)\n",
    "  # 불용어 제거\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  keywords = [(word, freq) for word, freq in counter.most_common(num_keywords) if word not in stop_words and len(word) > 1]\n",
    "  return keywords\n",
    "\n",
    "# 메인 함수\n",
    "def main () :\n",
    "  base_url = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "  press_news_data = get_news_links_by_press(base_url)\n",
    "  \n",
    "  # 뉴스 DataFrame 생성\n",
    "  news_list = []\n",
    "  for press_name, news_data in press_news_data.items() :\n",
    "    for title, link in news_data :\n",
    "      news_list.append([press_name, title, link])\n",
    "  df = pd.DataFrame(news_list, columns=['Press', 'Title', 'Link'])\n",
    "  \n",
    "  # Data 전처리\n",
    "  df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
    "  \n",
    "  # 감성 분석\n",
    "  df['Sentiment'] = df['Title'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n",
    "  \n",
    "  # Topic 모델링\n",
    "  lda, vectorizer = topic_modeling(df['Processed_Title'].tolist())\n",
    "  topics = lda.components_\n",
    "  feature_names = vectorizer.get_feature_names_out()\n",
    "  topic_keywords = []\n",
    "  for topic_weights in topics :\n",
    "    top_keywords = [feature_names[i] for i in topic_weights.argsort()[:-11:-1]]\n",
    "    topic_keywords.append(top_keywords)\n",
    "  \n",
    "  # 주요 Keyword 추출\n",
    "  keywords = extract_keywords(df['Processed_Title'].tolist())\n",
    "  \n",
    "  # 결과 출력\n",
    "  print(\"오늘의 주요 키워드:\")\n",
    "  for keyword, freq in keywords :\n",
    "    print(f\"{keyword}: {freq}\")\n",
    "  \n",
    "  print(\"\\n추천 뉴스:\")\n",
    "  for idx, row in df.head(10).iterrows() :  # 상위 10개의 뉴스만 출력\n",
    "    print(f\"{idx + 1}. {row['Title']} ({row['Sentiment']})\")\n",
    "    print(f\"   Link: {row['Link']}\")\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
