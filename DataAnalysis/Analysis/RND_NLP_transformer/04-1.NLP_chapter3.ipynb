{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### NLP chapter 3. Transformer deep 하게 이해하기.\n",
    "\n",
    "# 3.1 transformer architecture\n",
    "- 인코더 : 입력토큰의 시퀀스를 은닉상태 또는 문맥(context) 라 부르는 임베딩 벡터의 시퀀스로 반환\n",
    "- 디코더 :\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><img src = \"https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F914ff162-e9aa-4454-80d2-30b702551154%2F099316f3-3caa-4843-83c2-8c85ecd87382%2FUntitled.png?table=block&id=52e74767-4875-4af9-aab4-571a9ecd0bb1&spaceId=914ff162-e9aa-4454-80d2-30b702551154&width=1710&userId=d72aebc7-576c-4f0e-9c69-1d5d32da7874&cache=v2\" width=\"500\" height=\"300\"/><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m📌 - PROGRAM START \n",
      "\t\u001b[0m\n",
      "\u001b[93m📌 - MPS 장치를 지원 Build 여부 : \u001b[0m True\n",
      "\u001b[93m📌 - MPS 장치 사용가능 여부 : \u001b[0m True\n",
      "\u001b[93m📌 - GPU 사용Start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Forrest Park's notion  ## Updated 2024.07.03\n",
    "### Service  Text Coloring option\n",
    "from Functions import Service as S\n",
    "def blue(str):return S.colored_text(str,'blue')\n",
    "def yellow(str):return S.colored_text(str,'yellow')\n",
    "def red(str):return S.colored_text(str,'red')\n",
    "def green(str):return S.colored_text(str,'green')\n",
    "def imd(str):return S.imd(green(str))\n",
    "## 자연어처리 패키지 설치 \n",
    "def NLPInstalls():\n",
    "    import subprocess,sys\n",
    "    import warnings ; warnings.filterwarnings('ignore')\n",
    "    # pip가 없으면 pip를 설치\n",
    "    try:import pip\n",
    "    except ImportError:\n",
    "        print(\"Install pip for python3\")\n",
    "        subprocess.call(['sudo', 'apt-get', 'install', 'python3-pip'])\n",
    "    \n",
    "    # tweepy 없으면 tweepy 설치\n",
    "    try:import tweepy        \n",
    "    except ModuleNotFoundError:\n",
    "        print(\"Install tweepy\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'tweepy==3.10.0'])\n",
    "    finally:import tweepy \n",
    "    \n",
    "    # konlpy 없으면 konlpy 설치\n",
    "    try:import konlpy\n",
    "    except ModuleNotFoundError: \n",
    "        print(\"Install konlpy\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'konlpy'])\n",
    "    finally:import konlpy\n",
    "    \n",
    "    # eunjeon 없으면 eunjeon 설치\n",
    "    try:import eunjeon\n",
    "    except ModuleNotFoundError: \n",
    "        print(\"Install eunjeon : eunjeon\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'eunjeon'])\n",
    "    finally:import konlpy\n",
    "    \n",
    "    # datasets 없으면 datasets를 설치\n",
    "    try:import datasets\n",
    "    except ModuleNotFoundError: \n",
    "        print(\"Install datasets : datasets\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'datasets'])\n",
    "    finally:import datasets\n",
    "    \n",
    "    # pytorch 없으면 pytorch 설치\n",
    "    try:import torch\n",
    "    except ModuleNotFoundError: \n",
    "        print(\"Install torch : pytorch\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'pytorch'])\n",
    "    finally:import torch\n",
    "    \n",
    "    # transformers 없으면 transformers 설치\n",
    "    try:import transformers\n",
    "    except ModuleNotFoundError: \n",
    "        print(\"Install transformer : transformers\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'transformers'])\n",
    "    finally:import transformers\n",
    "        \n",
    "    # UMAP 없으면 UMAP 설치\n",
    "    try:import umap\n",
    "    except ModuleNotFoundError: \n",
    "        print(\"Install umap : umap\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'umap'])\n",
    "    finally:import umap\n",
    "        \n",
    "    # UMAP 없으면 UMAP 설치\n",
    "    try:from umap import UMAP\n",
    "    except ImportError: \n",
    "        print(\"Install umap : umap-learn\")\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", 'umap-learn'])\n",
    "    finally:import umap\n",
    "NLPInstalls()\n",
    "print(yellow(f\"📌 - PROGRAM START \\n\\t\"))\n",
    "## GPU setting (in MacOS)\n",
    "import torch\n",
    "print(yellow(f\"📌 - MPS 장치를 지원 Build 여부 : \"),torch.backends.mps.is_built())\n",
    "print(yellow(f\"📌 - MPS 장치 사용가능 여부 : \"),torch.backends.mps.is_available())\n",
    "print(yellow(f\"📌 - GPU 사용Start\"))\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#### Jupyter Basic Setting ####\n",
    "import pandas as pd,numpy as np\n",
    "import matplotlib.pyplot as plt,seaborn as sns  # 시각화\n",
    "import warnings; warnings.filterwarnings('ignore')  # 경고 무시\n",
    "import sys,os \n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<br><img src = \"https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F914ff162-e9aa-4454-80d2-30b702551154%2F099316f3-3caa-4843-83c2-8c85ecd87382%2FUntitled.png?table=block&id=52e74767-4875-4af9-aab4-571a9ecd0bb1&spaceId=914ff162-e9aa-4454-80d2-30b702551154&width=1710&userId=d72aebc7-576c-4f0e-9c69-1d5d32da7874&cache=v2\" width=\"500\" height=\"300\"/><br>\n"
     ]
    }
   ],
   "source": [
    "#<img src = \"\" width=\"300\" height=\"300\"/><br>\n",
    "def imd(image_address,width =500, height=300):\n",
    "    print(f'<br><img src = \"{image_address}\" width=\"{width}\" height=\"{height}\"/><br>')\n",
    "    \n",
    "imd(\"https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F914ff162-e9aa-4454-80d2-30b702551154%2F099316f3-3caa-4843-83c2-8c85ecd87382%2FUntitled.png?table=block&id=52e74767-4875-4af9-aab4-571a9ecd0bb1&spaceId=914ff162-e9aa-4454-80d2-30b702551154&width=1710&userId=d72aebc7-576c-4f0e-9c69-1d5d32da7874&cache=v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformer 의 유형\n",
    "- 인코더 유형 : \n",
    "    * 텍스트 시퀀스 입력을 풍부한 수치표현으로 변환 \n",
    "    * 텍스트 분류나 개체명 인식 같은 작업에 잘맞음. \n",
    "    * Bert, RoBert\n",
    "- 디코더 유형 :\n",
    "    * 시작 텍스트가 주어지면 가장 가능성 있는 다음 단어를 반복해 예측하는 식으로 시퀀스 자동완성\n",
    "    * GPT , causal attention, autoregressive attention\n",
    "    * 한 토큰에 대해 계산한 표현은 오직 왼쪽 문맥에 따라 달라짐. \n",
    "\n",
    "- 인코더-디코더 유형: \n",
    "    * 한텍스트의 시퀀스를 다른 시퀀스로 매핑하는 복잡한 모델링에 사용 \n",
    "    * 기계번역과 요약작업에 적합. \n",
    "    * BART, T5 모델 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 인코더\n",
    ": 인코더층은 임베딩 시퀀스를 받아 다음 두 층에 통과시킨다.      \n",
    ": 임베딩이란 자연어처리에서 사람이 쓰는 자연어를 기계가 이해할 수 있도록 숫자형태인 vector로 바꾸는 과정 혹은 일련의 전체 과정을 의미\n",
    "- multihead self attention layer\n",
    "- fully connected feed forward layer (각각의 입력 임베딩에 적용되는 완전 연결 피드 포워드층 )\n",
    "<br><img src = \"https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F914ff162-e9aa-4454-80d2-30b702551154%2F09d456af-00b9-4ab3-8a84-7fd2c463d27e%2FUntitled.png?table=block&id=eec10382-9a45-4f26-ae3f-9787fa035e64&spaceId=914ff162-e9aa-4454-80d2-30b702551154&width=1710&userId=d72aebc7-576c-4f0e-9c69-1d5d32da7874&cache=v2\" width=\"500\" height=\"300\"/><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 3.2.1  self attention \n",
    "- 텍스 시퀀스에서 원소는 토큰 임베딩임. 각 토큰은 고정차원의 벡터에 매핑됨. \n",
    "- 각 토큰에 대해 고정된 임베딩을 사용하는 대신 전체 시퀀스를 사용해  각 임베딩의 가중평균을 계산하는것이 셀프 어텐션의 기본 개념 \n",
    "\n",
    "- 토큰임베딩 시퀀스 x1,x2,...,xn -> 새로운 임베딩 시퀀스 x'1,x'2,...\n",
    "- 계수 wji -> 어텐션 가중치  전체합은 1 \n",
    "\n",
    "- 토큰 임베딩의 평균을 구하는 이유?\n",
    "    : 문맥 고려 임베딩 (contextualized embadding)\n",
    "    * ELMo 언어 모델에서 시작 됨. \n",
    "    <br><img src = \"https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F914ff162-e9aa-4454-80d2-30b702551154%2Fd8492d8f-8127-4900-a162-e70dc7f2ce84%2FUntitled.png?table=block&id=61f37201-01a3-4515-bea4-161b2153733c&spaceId=914ff162-e9aa-4454-80d2-30b702551154&width=1710&userId=d72aebc7-576c-4f0e-9c69-1d5d32da7874&cache=v2\" width=\"500\" height=\"300\"/><br>\n",
    "\n",
    "\n",
    "### 스케일드 점곱 어텐션 \n",
    "- scaled dot-product attention \n",
    ": \n",
    "- 각 토큰 임베딩을 쿼리(query) , 키(Key), 값(Value) 세개의 벡터로 투영합니다. \n",
    "- 어텐션 점수를 계산 , 유사도 함수를 사용해 쿼리 벡터와 키벡터가 서로 얼마나 관련되는지 계산\n",
    "attention score = 어텐션 점수 행렬 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b204a75f424042a83e868e35aa8a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfa01d150ad43f19dca150da786700a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694fc0de65d34bc78902a8fd85e0d85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a1e8f6c7c845ca978a24e0e82ee653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:00<00:00, 935669.05B/s]\n",
      " 33%|███▎      | 147081216/440473133 [00:32<00:54, 5375748.69B/s]"
     ]
    }
   ],
   "source": [
    "# !pip install bertviz\n",
    "from transformers import AutoTokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "\n",
    "model_ckpt =\"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "text = 'time flies like an arrow'\n",
    "show(model, 'bert', tokenizer, text, display_mode='light', layer=0, head=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertviz\n",
      "  Downloading bertviz-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from bertviz) (4.41.2)\n",
      "Requirement already satisfied: torch>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from bertviz) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from bertviz) (4.66.4)\n",
      "Collecting boto3 (from bertviz)\n",
      "  Downloading boto3-1.34.139-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from bertviz) (2.32.3)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.11/site-packages (from bertviz) (2023.10.3)\n",
      "Collecting sentencepiece (from bertviz)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.4.3)\n",
      "Collecting botocore<1.35.0,>=1.34.139 (from boto3->bertviz)\n",
      "  Downloading botocore-1.34.139-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from boto3->bertviz) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->bertviz)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.139->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.139->boto3->bertviz) (1.16.0)\n",
      "Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.34.139-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.139-py3-none-any.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, botocore, s3transfer, boto3, bertviz\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.34.139 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bertviz-1.4.0 boto3-1.34.139 botocore-1.34.139 s3transfer-0.10.2 sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
